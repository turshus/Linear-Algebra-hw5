\documentclass[]{exam}

\usepackage{amsmath, amssymb}
\usepackage[margin=1in]{geometry}
\usepackage[en-US, showdow]{datetime2}
\usepackage{bm}
\usepackage{hyperref}


\title{Homework 5\\
	Due \DTMdate{2022-02-11} %Add a % at the begining of the line to remove the due date from the title (or just delete this line entirely) 
	}
\author{Connor Meads - A01853906}
\begin{document}
\maketitle

\printanswers



\begin{questions}
		\question Consider the statements about symmetric matrices and indicate if the statements are true or false. If a statement is true provide a proof, otherwise give a counterexample. 
		
		\begin{parts}
			\part The block matrix $\begin{bmatrix} 0 & A\\ A & 0 \end{bmatrix}$ is automatically symmetric.
			\begin{solution}
				This is true.  A great way to test symmetry is to simply take the transpose of the matrix.  If they are the same, then they are symmetric.
				\[
					A = \begin{bmatrix}
						0 & A \\
						A & 0
					\end{bmatrix}	
				\]
				\[
					A^T = 	\begin{bmatrix}
						0 & A \\
						A & 0
					\end{bmatrix}
				\]
				Since we can conclude that $A = A^T$, we can conclude that this is symmetric.
			\end{solution}
			\part If $A$ and $B$ are symmetric then $AB$ is symmetric.
			\begin{solution}
				Besides for a few specific examples, this is generally false.  Multiplying two symmetric matrices will most often result in a non-symmetric matrix.
				\[
					A = \begin{bmatrix}
						1 & 2 & 3 \\
						2 & 1 & 2 \\
						3 & 2 & 1
					\end{bmatrix}	
				\]
				\[
					B = \begin{bmatrix}
						3 & 2 & 1 \\
						2 & 3 & 2 \\
						1 & 2 & 3
					\end{bmatrix}	
				\]
				\[
					AB = \begin{bmatrix}
						1 & 2 & 3 \\
						2 & 1 & 2 \\
						3 & 2 & 1
					\end{bmatrix}
					\begin{bmatrix}
						3 & 2 & 1 \\
						2 & 3 & 2 \\
						1 & 2 & 3
					\end{bmatrix} =
					\begin{bmatrix}
						10 & 14 & 14 \\
						10 & 11 & 10 \\
						14 & 14 & 10
					\end{bmatrix}
				\]
				We can see that $A$ and $B$ are clearly symmetric as they both equal their transposes.  Their product does not.
				\[
					\begin{bmatrix}
						10 & 14 & 14 \\
						10 & 11 & 10 \\
						14 & 14 & 10
					\end{bmatrix} \neq
					\begin{bmatrix}
						10 & 10 & 14 \\
						14 & 11 & 14 \\
						14 & 10 & 10
					\end{bmatrix}
				\] 
				\[AB \neq AB^T\]
			\end{solution}
			\newpage
			\part If $A$ is not symmetric, then $A^{-1}$ is not symmetric.
			\begin{solution}
				This is true.  The inverse of matrix $A$ will have a negative value of itself in the same spot meaning that if matrix $A$ was not symmetric, it would be impossible for $A^{-1}$ to be.
			\end{solution}
			\part If $A$, $B$, and $C$ are symmetric, then $(ABC)^T=CBA$
			\begin{solution}
				This is true.  We can think of this as simply flipping the order in which we multiply the matrices or just flipping the final result.
				\[
					A = \begin{bmatrix}
						1 & 2 & 3 \\
						2 & 1 & 2 \\
						3 & 2 & 1
					\end{bmatrix}
					B = \begin{bmatrix}
						3 & 2 & 1 \\
						2 & 3 & 2 \\
						1 & 2 & 3
					\end{bmatrix}
					C = \begin{bmatrix}
						2 & 3 & 1 \\
						3 & 2 & 3 \\
						1 & 3 & 2
					\end{bmatrix}	
				\]
				\[
					(ABC) =
					\begin{bmatrix}
						1 & 2 & 3 \\
						2 & 1 & 2 \\
						3 & 2 & 1
					\end{bmatrix}
					\begin{bmatrix}
						3 & 2 & 1 \\
						2 & 3 & 2 \\
						1 & 2 & 3
					\end{bmatrix}
					\begin{bmatrix}
						2 & 3 & 1 \\
						3 & 2 & 3 \\
						1 & 3 & 2
					\end{bmatrix}	=
					\begin{bmatrix}
						76 & 100 & 80 \\
						63 & 82 & 63 \\
						80 & 100 & 76
					\end{bmatrix}	
				\]
				\[
					(ABC)^T = \begin{bmatrix}
						76 & 63 & 80 \\
						100 & 82 & 100 \\
						80 & 63 & 76
					\end{bmatrix}
				\]
				Now we can solve for $CBA$ and compare the two.
				\[
					\begin{bmatrix}
						2 & 3 & 1 \\
						3 & 2 & 3 \\
						1 & 3 & 2
					\end{bmatrix}
					\begin{bmatrix}
						3 & 2 & 1 \\
						2 & 3 & 2 \\
						1 & 2 & 3
					\end{bmatrix}
					\begin{bmatrix}
						1 & 2 & 3 \\
						2 & 1 & 2 \\
						3 & 2 & 1
					\end{bmatrix} =
					\begin{bmatrix}
						76 & 63 & 80 \\
						100 & 82 & 100 \\
						80 & 63 & 76
					\end{bmatrix}
				\]
			\end{solution}
		\end{parts}
		
		\newpage
		\question \S 3.1 \# 10. Which of the following subsets of $\mathbb{R}^3$ are actually subspaces? You should either prove that the set is a subspace or show that the set does not have one of the properties of subspaces.
	
	\begin{parts}
		\part The plane of vectors $(b_1, b_2, b_3)$ with $b_1 = b_2$.
		\begin{solution}
			We can re-write the plane of vectors to more accurately represent our constraints:
            \[
                \begin{bmatrix}
                    b_1 & b_1 & b_2
                \end{bmatrix}
            \]
            This is a subset of $\mathbb{R}^3$ and we can prove it by proving that the sum of any two vectors within the subsetis also in the subset:
            \[
                \begin{bmatrix}
                    b_1 \\ b_1 \\ b_2
                \end{bmatrix} +
                \begin{bmatrix}
                    b_3 \\ b_3 \\ b_4
                \end{bmatrix} =
                \begin{bmatrix}
                    b_1 + b_3 \\
                    b_1 + b_3 \\
                    b_2 + b_4
                \end{bmatrix}
            \]
            As we can see, the first two elemetns of the vectors equal eqch other which satisfies this constraint.  We also need to prove that any scalar multiple of a vector in the subset is also in the subset.
            \[
                \alpha \begin{bmatrix}
                    b_1 \\ b_1 \\ b_2
                \end{bmatrix} +
                \beta \begin{bmatrix}
                    b_3 \\ b_3 \\ b_4
                \end{bmatrix}
            \]
            \[
                \begin{bmatrix}
                    \alpha \cdot b_1 \\
                    \alpha \cdot b_1 \\
                    \alpha \cdot b_2
                \end{bmatrix} +
                \begin{bmatrix}
                    \beta \cdot b_3 \\
                    \beta \cdot b_3 \\
                    \beta \cdot b_4
                \end{bmatrix}
            \]
		\end{solution}
		\part The plane of vectors with $b_1 = 1$.
		\begin{solution}
			This plane is not a subspace of $\mathbb{R}^3$ as it fails both the addition and multiplication tests.
			\[
				\begin{bmatrix}
					1 \\ a_1 \\ a_2
				\end{bmatrix} +
				\begin{bmatrix}
					1 \\ b_1 \\ b_2
				\end{bmatrix} =
				\begin{bmatrix}
					1 + 1 \\
					a_1 + b_1 \\
					a_2 + b_2
				\end{bmatrix} =
				\begin{bmatrix}
					2 \\ 
					a_1 + b_1 \\
					a_2 + b_2
				\end{bmatrix}
			\]
			This fails as $b_1$ adds together to become that fails the constraint.
		\end{solution}
		\part The vectors $(b_1, b_2, b_3)$ with $b_1b_2b_3 = 0$.
		\begin{solution}
			This is false.  The only scenario where this would prove true is if one of the vectors was the zero vector, however, there are many examples of vectors that are non-zero.
		\end{solution}
		\part All linear combinations of $\vec{v} = (1,4,0)$ and $\vec{w} = (2,2,2)$.
		\begin{solution}
			This is true.  We could write this in the form $\vec{s} = \alpha\vec{v}+\beta\vec{w}$ where $\alpha$ and $\beta$ are any scalar.  These parameters describe the subspace it exists in.
		\end{solution}
		\part All vectors $(b_1, b_2, b_3)$ with $b_1 + b_2 + b_3 = 0$.
		\begin{solution}
			This is true.  Adding different vectors that follow this principle will always have their result follow the same principle. Let's look at an example:
			\[
				\begin{bmatrix}
					4 & 7 & -11
				\end{bmatrix} +
				\begin{bmatrix}
					3 & -1 & -2
				\end{bmatrix} = 
				\begin{bmatrix}
					7 & 6 & -13
				\end{bmatrix}
			\]
			As we can see the resulting vector follows the same rules.  We can now test it for multiplication:
			\[
				\alpha\begin{bmatrix}
					4 & 7 & -11
				\end{bmatrix} +
				\beta\begin{bmatrix}
					3 & -1 & -2
				\end{bmatrix} = 
				\begin{bmatrix}
					4\alpha + 3\beta \\
					7\alpha - \beta \\
					-11\alpha - 2\beta
				\end{bmatrix}
			\]
			\[
				(4\alpha + 3\beta) + (7\alpha - \beta) + (-11\alpha - 2\beta) = \alpha(4 + 7 - 11) + \beta(3 - 1 - 2) = \alpha \cdot 0 + \beta \cdot 0 = 0
			\]
			This is also true, thus this passes.
		\end{solution}
		\part All vectors $(b_1, b_2, b_3)$ with $b_1 \leq b_2 \leq b_3$.
		\begin{solution}
			This is false.  For all positive values it would be true, however, imagine if any of these values were negative.
			\[
				-2 \cdot \begin{bmatrix}
					-3 & 1 & 2
				\end{bmatrix}
				=
				\begin{bmatrix}
					6 & -2 & -4
				\end{bmatrix}
			\]
			This has effectively reversed the order and proved this false.
		\end{solution}
	\end{parts}

	\question An exercise using the outer-product method of matrix multiplication. Every matrix with rank $r$ can be written as the sum of $r$ rank 1 matices. An easy way to write a rank 1 matrix is using an outer-product (recall: $\vec{u}\vec{v}^T$ is an outer-product). Construct a matrix $A$ with rank 2 that has $C(A) = span((1,2,4), (2,2,1))$ and $C(A^T) = span((1,0),(1,1))$, you should use outer-products to find $A$. Then find a factorization on $A$ into a 3 by 2 matrix times a 2 by 2 matrix, you should think backwards about the outer product method of matrix multiplication to help you.

\begin{solution}
	We can begin to find matrix $A$ by multiplying the column space by the row space:
	\[
		\begin{bmatrix}
			1 \\ 2 \\ 4
		\end{bmatrix} \cdot
		\begin{bmatrix}
			1 & 0
		\end{bmatrix} = 
		\begin{bmatrix}
			1 & 0 \\
			2 & 0 \\
			4 & 0 
		\end{bmatrix}
	\]
	\[
		\begin{bmatrix}
			2 \\ 2 \\ 1
		\end{bmatrix} \cdot 
		\begin{bmatrix}
			1 & 1
		\end{bmatrix} =
		\begin{bmatrix}
			2 & 2 \\
			2 & 2 \\
			1 & 1
		\end{bmatrix}
	\]
	These products are always going to be legal dimensions to add by:
	\[
		\begin{bmatrix}
			1 & 0 \\
			2 & 0 \\
			4 & 0 
		\end{bmatrix} +
		\begin{bmatrix}
			2 & 2 \\
			2 & 2 \\
			1 & 1
		\end{bmatrix} = 
		\begin{bmatrix}
			3 & 2 \\
			4 & 2 \\
			5 & 1
		\end{bmatrix}
	\]
	Clearly we can see that column 2 of that result is in the column space.  Column 1 is also in the column space.
	Now we can use the outer-product method of matrix multiplication:
	\[
		c_1\cdot r_1 + c_2\cdot r_2	
	\]
	\[
		\begin{bmatrix}
			1 \\ 2 \\ 4
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0
		\end{bmatrix} +
		\begin{bmatrix}
			2 \\ 2 \\ 1
		\end{bmatrix}
		\begin{bmatrix}
			1 & 1
		\end{bmatrix}
		=
		\begin{bmatrix}
			1 & 2 \\
			2 & 2 \\
			4 & 1
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0 \\
			1 & 1
		\end{bmatrix} = 
		\begin{bmatrix}
			3 & 2 \\
			4 & 2 \\
			5 & 1
		\end{bmatrix}
	\]
\end{solution}
\newpage
\question Suppose that $A$ is a $m \times n$ matrix and $\vec{b}$ is a $m \times 1$ vector. Let $B$ be the $m \times (n+1)$ matrix formed by adding $\vec{b}$ to $A$, so $B = [A \, \vec{b}]$. What must be true so that $C(A)=C(B)$? What must be true if $C(A)$ is smaller that $C(B)$? Explain what must be true for $A \vec{x} = \vec{b}$ and $B \vec{x} = \vec{b}$ to have solutions. 


\begin{solution}
	In order for $C(A) = C(B)$, $\vec{b}$ must be a linear combination of some column in matrix $A$.  In order for $C(A) < C(B)$, $\vec{b}$ must be linearly independent of any columns in matrix $A$.  In order for $A\vec{x}=\vec{b}$ \emph{and} $B\vec{x}=\vec{b}$ to both have solutions, $\vec{b}$ simply has to be a linear combination of some column in its corresponding matrix.  For example:
	\[
		A = \begin{bmatrix}
			a & b & c \\
			d & e & f \\
			g & h & i
		\end{bmatrix}
		\begin{bmatrix}
			0 \\ 0 \\ 1
		\end{bmatrix} =
		\begin{bmatrix}
			c \\ f \\ i
		\end{bmatrix}
	\]
	\[
		B = \begin{bmatrix}
			a & b & c & j \\
			d & e & f & k \\
			g & h & i & l
		\end{bmatrix}
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 1
		\end{bmatrix} =
		\begin{bmatrix}
			j \\ k \\ l
		\end{bmatrix}
	\]
\end{solution}

\newpage
\question Suppose that $A = 
\begin{bmatrix}
	1 & 2 & 0 & 1\\
	-1 & -2 & 1 & 0\\
	2 & 4 & 0 & 2
\end{bmatrix}$
\begin{parts}
	

	\part Describe the column space of $A$ by listing a basis for it.
		
	\begin{solution}
		The column space can be found by finding the pivots after elemination, so let's elimniate!
		\[
			EA = 
			\begin{bmatrix}
				1 & 0 & 0 \\
				1 & 1 & 0 \\
				-2 & 0 & 1
			\end{bmatrix}
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				-1 & -2 & 1 & 0 \\
				2 & 4 & 0 & 2
			\end{bmatrix} =
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				0 & 0 & 1 & 1 \\
				0 & 0 & 0 & 0
			\end{bmatrix}
		\]
		It's clear to see now that our pivots are in $x_1$ and $x_3$.  The basis for the column space $A$ can be described by the columns that contain the pivots.
		\[
			C(A) = span\left(\left\{
				\begin{bmatrix}
					1 \\ -1 \\ 2
				\end{bmatrix} ,
				\begin{bmatrix}
					0 \\ 1 \\ 0
				\end{bmatrix}
			\right\}\right)	
		\]
	\end{solution}
	
	\part Describe the nullspace of $A$ by listing a basis for it.
	
	\begin{solution}
		We can start to find the nullspace by referencing our elemination matrix $E$ that we solved above.
		\[
			EA = 
			\begin{bmatrix}
				1 & 0 & 0 \\
				1 & 1 & 0 \\
				-2 & 0 & 1
			\end{bmatrix}
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				-1 & -2 & 1 & 0 \\
				2 & 4 & 0 & 2
			\end{bmatrix} =
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				0 & 0 & 1 & 1 \\
				0 & 0 & 0 & 0
			\end{bmatrix}
		\]
		We can see by looking at the matrix $EA$ that we have pivots at $x_1$ and $x_3$ which means that our free variables are $x_2$ and $x_4$.  Let's set $x_2 = 1$ and $x_4 = 0$.
		\[
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				0 & 0 & 1 & 1 \\
				0 & 0 & 0 & 0
			\end{bmatrix} =
			\begin{array}{r}
				x_1 + 2x_2 + x_4 = 0 \\
				x_3 + x_4 = 0
			\end{array}
		\]
		\[
			\begin{array}{c}
				x_1 + 2(1) + 0 = 0 \rightarrow x_1 = -2	\\
				x_3 + 0 = 0 \rightarrow x_3 = 0
			\end{array}
		\]
		We now have one solution. We can switch the values of $x_2$ and $x_4$ to get our 2nd solution.
		\[
			\begin{array}{c}
				x_1 + 2(0) + 1 = 0 \rightarrow x_1 = -1	\\
				x_3 + 1 = 0 \rightarrow x_3 = -1
			\end{array}
		\]
		We can now describe the nullspace with the basis for it:
		\[
			N(A) = span\left(\left\{
				\begin{bmatrix}
					-1 \\ 0 \\ -1 \\1
				\end{bmatrix},\begin{bmatrix}
					-2 \\ 1 \\ 0 \\ 0
			\end{bmatrix}
			\right\}\right)
		\]
	\end{solution}
	\newpage
	\part Describe the left nullspace of $A$ by listing a basis for it.
	
	\begin{solution}
		We can find the left nullspace of $A$ by referencing the elemination we did on $A$ in the problem above.
		\[
			EA = 
			\begin{bmatrix}
				1 & 0 & 0 \\
				1 & 1 & 0 \\
				-2 & 0 & 1
			\end{bmatrix}
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				-1 & -2 & 1 & 0 \\
				2 & 4 & 0 & 2
			\end{bmatrix} =
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				0 & 0 & 1 & 1 \\
				0 & 0 & 0 & 0
			\end{bmatrix}
		\]
		Pay attention to row 3 in $EA$.  It is all 0's.  Because the left nullspace is just the transpose of the nullspace, we can think of the rows as columns and vice versa.  This would mean that row 3 in $EA$ would actually be column 3 in $EA^T$.  This would mean that the third row in our elemination matrix $E$ would be our only solution to the left nullspace.
		\[
			N(A)^T	= span \left(\left\{
				\begin{bmatrix}
					-2 \\ 0 \\ 1
				\end{bmatrix}
				\right\}\right)
		\]
	\end{solution}
	
	\part Describe the row space of $A$ by listing a basis for it.
	
	\begin{solution}
		Much like finding the column space, we can start by finding $EA$:
		\[
			EA = 
			\begin{bmatrix}
				1 & 0 & 0 \\
				1 & 1 & 0 \\
				-2 & 0 & 1
			\end{bmatrix}
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				-1 & -2 & 1 & 0 \\
				2 & 4 & 0 & 2
			\end{bmatrix} =
			\begin{bmatrix}
				1 & 2 & 0 & 1 \\
				0 & 0 & 1 & 1 \\
				0 & 0 & 0 & 0
			\end{bmatrix}
		\]
		Now we simply pay attention to where the pivots are located in the rows, and those are our answers.  The pivots are located at $x_1$ and $x_3$.
		\[
			C(A)^T = span \left(\left\{
				\begin{bmatrix}
					1 & 2 & 0 & 1
				\end{bmatrix},
				\begin{bmatrix}
					0 & 0 & 1 & 1
				\end{bmatrix}
			\right\}\right)	
		\]
	\end{solution}
\end{parts}


\end{questions}



\end{document}